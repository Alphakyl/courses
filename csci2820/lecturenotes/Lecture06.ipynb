{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mymat}[1]{\n",
    "\\left[\n",
    "\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right]\n",
    "}\n",
    "\\newcommand{\\myaug}[1]{\n",
    "\\left[\n",
    "\\begin{array}{rrr|r}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right]\n",
    "}\n",
    "\\newcommand{\\myp}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\myb}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\myv}[1]{\\left< #1 \\right>}\n",
    "\\newcommand{\\mys}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\myab}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\bx}{{\\bf x}}\n",
    "\\newcommand{\\by}{{\\bf y}}\n",
    "\\newcommand{\\bu}{{\\bf u}}\n",
    "\\newcommand{\\bv}{{\\bf v}}\n",
    "\\newcommand{\\be}{{\\bf e}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 6\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Last time we saw that the elimination matrix $E_{31} = \\mymat{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -3 & 0 & 1}$ and the matrix $L_{31} = \\mymat{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 3 & 0 & 1}$ shared a special relationship.\n",
    "\n",
    "<br>\n",
    "\n",
    "When we multiply the two matrices together (in any order) the result is the identity matrix: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "E_{31}L_{31} = L_{31}E_{31} = I \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$E_{31}$ and $L_{31}$ are inverses of each other.  Put in the usual way, $L_{31}$ is the inverse of $E_{31}$, denoted \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "E_{31}^{-1} = L_{31}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Definition**: A square matrix $A$ is **invertible** if there exists a square matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$\n",
    "\n",
    "OK, so how are inverses useful?  Suppose we want to solve a linear system of the form $A\\bx = {\\bf b}$: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bx = {\\bf b} \\quad \\Rightarrow \\quad A^{-1}A\\bx = A^{-1}{\\bf b} \\quad \\Rightarrow \\quad I\\bx = A^{-1}{\\bf b} \\quad \\Rightarrow \\quad \\bx = A^{-1}{\\bf b}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This shows that if we have access to $A^{-1}$ then multiplying the rhs vector ${\\bf b}$ by the inverse gives the solution to the linear system.  We have other (better) ways of solving linear systems, namely Gaussian Elimination and the LU Decomposition.  When we use one of these other methods to solve the linear system we often describe it as *inverting* the linear system, or *applying the action of* $A^{_1}$.  In practical application we almost never compute $A^{-1}$ explicitly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOT ALL MATRICES HAVE INVERSES!!**\n",
    "\n",
    "You've all seen the idea of an *inverse function* in calculus.  Consider the function $f(x) = x^3$.  If we know that $f(a) = 8$ then we know that $a$ must be equal to $2$. The function that takes us from 8 to 2 is called the inverse of $f$.  In this case it's $f^{-1}(y) = y^{1/3}$ and for the example we could write $f^{-1}(8) = 2$.  \n",
    "\n",
    "But consider another example with $g(x) = x^2$.  If we know that $g(a) = 4$ then $a$ could be either $2$ or $-2$.  Remember that scalar functions are things that take one input and return one output.  Since an inverse function for $g$ would have to take one input (4) and return two outputs (2 and -2) the function is undefined.  Thus $g = x^2$ has **no inverse**. \n",
    "\n",
    "OK, back to linear algebra.  Suppose that you're given a coefficient matrix $A$ and a rhs vector ${\\bf b}$ and we want to solve the linear system $A\\bx = {\\bf b}$.  Do we know exactly (i.e. uniquely) what $\\bx$ is? \n",
    "\n",
    "The answer of course is that it depends on the coefficient matrix $A$.  If $A$ is nonsingular then the linear system has a unique solution vector $\\bx$ for each rhs vector ${\\bf b}$ and thus $A$ is invertible.  On the other hand if $A$ is singular then a given rhs vector ${\\bf b}$ either has no corresponding solution $\\bx$ or it has infinitely many.  In this case the coefficient matrix $A$ is not invertible.  \n",
    "\n",
    "For a given $n \\times n$ matrix the following statements are equivalent: \n",
    "\n",
    "* $A$ is invertible\n",
    "* $A$ is nonsingular\n",
    "* The elimination process on $A$ yields $n$ pivots\n",
    "\n",
    "Here are a bunch of facts about matrix inverses. \n",
    "\n",
    "**Fact 1**: If $A$ is invertible then it has exactly one inverse. \n",
    "\n",
    "**Proof**: The standard pattern for proving that something is unique is to assume that there are two of them and then show that they were really the same all along.  Suppose matrices $B$ and $C$ are both inverses of $A$.  Then we have the following \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "BAC = BAC \\quad \\Rightarrow \\quad (BA)C = B(AC) \\quad \\Rightarrow \\quad IC = BI \\quad \\Rightarrow \\quad C = B\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Fact 2**: If there exists a nonzero vector $\\bx$ such that $A \\bx = {\\bf 0}$ then $A$ is not invertible. \n",
    "\n",
    "**Proof**: Recall that a linear system $A\\bx = {\\bf b}$ has either one solution, no solutions, or infinitely many solutions.  There are no inbetweens.  Notice then that clearly $\\bx = {\\bf 0}$ is a solution to $A \\bx = {\\bf 0}$.  If $\\bx \\neq {\\bf 0}$ also solves $A\\bx = {\\bf 0}$ then we've found two solutions, which means there must be infinitely many.  Since there are infinitely many solutions the matrix $A$ is singular and thus not invertible. \n",
    "\n",
    "**Fact 3**: If $A$ and $B$ are both $n \\times n$ invertible matrices then so is their product $(AB)$ and it's inverse is given by \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "(AB)^{-1} = B^{-1}A^{-1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Proof**: We have the following: \n",
    "\n",
    "<br> \n",
    "\n",
    "$$\n",
    "(B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = B^{-1}IB = B^{-1}B = I  \n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "(AB)(B^{-1}A^{-1}) =  A(BB^{-1})A^{-1} =AIA^{-1} =AA^{-1} = I\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Computing Inverses\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider the so-called *canonical basis vectors* in $\\mathbb{R}^3$.  These vectors are charactized by having a 1 in one position of the vector and zeros everywhere else. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\be_1 = \\mymat{1 \\\\ 0 \\\\ 0} \\quad \\be_2 = \\mymat{0 \\\\ 1 \\\\ 0} \\quad \\be_3 = \\mymat{0 \\\\ 0 \\\\ 1} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "What happens when you multiply one of the basis vectors by a matrix? \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mymat{\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "}\n",
    "\\mymat{\n",
    "1 \\\\ \n",
    "0 \\\\\n",
    "0 \n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "1 \\\\ \n",
    "4 \\\\\n",
    "7\n",
    "}\n",
    "\\quad\\quad\n",
    "\\mymat{\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "}\n",
    "\\mymat{\n",
    "0 \\\\ \n",
    "1 \\\\\n",
    "0 \n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "2 \\\\ \n",
    "5 \\\\\n",
    "8\n",
    "}\n",
    "\\quad\\quad\n",
    "\\mymat{\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "}\n",
    "\\mymat{\n",
    "0 \\\\ \n",
    "0 \\\\\n",
    "1 \n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "3 \\\\ \n",
    "6 \\\\\n",
    "9\n",
    "}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "OK, so multiplying a matrix by the $k^{\\textrm{th}}$ canonical basis vector picks out the $k^{\\textrm{th}}$ column of the matrix. Let's see how we could use this to our advantage in compute an inverse matrix.  \n",
    "\n",
    "Supposed for just a second that we **knew** the matrix $A^{-1}$.  Then multiplying it by the first canonical basis vector would pick out the first column of $A^{-1}$.  Of course, this is silly, because we don't know $A^{-1}$.  But, let's give this matrix product a name, say, $\\bx_1$.  \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_1 = A^{-1}\\be_1 \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Multiplying both sides of the equation by $A$ gives us \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_1 = A^{-1}\\be_1 \\quad \\Rightarrow \\quad A\\bx_1 = AA^{-1}\\be_1 \\quad \\Rightarrow \\quad  A\\bx_1 = \\be_1\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "OK, so remember that $\\bx_1$ is the first column of $A^{-1}$.  We've rewritten the problem so that if we want to *find* the first column of $A^{-1}$ we just need to solve the above linear system. Similarly, we could find the second column by solving $A\\bx_2 = \\be_3$ and the third column by solving $A\\bx_3 = \\be_3$.  \n",
    "\n",
    "So we can find the columns of $A^{-1}$ one-by-one just by solving linear systems, which we know how to do.  In fact, one easy way to compute the columns of $A^{-1}$ is to compute the LU Decomposition of $A$ and use it to quickly solve linear systems with the canonical basis vectors as the rhs vectors. \n",
    "\n",
    "While the method outlined above is probably the most pratical way to compute $A^{-1}$ the traditional method taught in all introductory linear algebra classes is the following. \n",
    "\n",
    "###Inverses by Gauss-Jordan Elimination\n",
    "\n",
    "OK, so we've seen that we can get the columns of $A^{-1}$ by solving linear systems of the form $A\\bx_k = \\be_k$.  The traditional method for finding the inverse matrix involves creating a large agumented system that solves all of these simultaneously.  \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "AA^{-1} = A\\mymat{\\bx_1 & \\bx_2 & \\bx_3} = \\mymat{\\be_1 & \\be_2 & \\be_3} = I\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We can do this by forming the large augmented system $\\left[ ~A ~ \\left| \\right. ~I~\\right]$ and then performing row reduction until the system looks like $\\left[ ~I~ \\left| \\right. ~A^{-1}~\\right]$.\n",
    "\n",
    "We accomplish this using the following steps: \n",
    "\n",
    "1. Row-reduce until $A$ becomes upper triangular $U$ (as usual)\n",
    "2. Row-reduce until $U$ becomes diagaonl $D$ \n",
    "3. Scale rows until $D$ becomes $I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Example 1\n",
    "\n",
    "Find $A^{-1}$ for $A = \\mymat{1 & -3 & 2 \\\\ -2 & 7 & -5 \\\\ 4 & -10 & 7}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & -3 & 2 & 1 & 0 & 0 \\\\ \n",
    "-2 & 7 & -5 & 0 & 1 & 0\\\\ \n",
    "4 & -10 & 7 & 0 & 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "~\\\\\n",
    "R_2 \\leftarrow R_2 + 2R_1 \\\\\n",
    "R_3 \\leftarrow R_3 - 4R_1\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & -3 & 2 & 1 & 0 & 0 \\\\ \n",
    "0 & 1 & -1 & 2 & 1 & 0\\\\ \n",
    "0 & 2 & -1 & -4 & 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "~\\\\\n",
    "~\\\\\n",
    "R_3 \\leftarrow R_3 - 2R_2\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & -3 & 2 & 1 & 0 & 0 \\\\ \n",
    "0 & 1 & -1 & 2 & 1 & 0\\\\ \n",
    "0 & 0 &  1 & -8 & -2 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "OK, so far we've just done vanilla Gaussian Elimination to transform the coefficient matrix into upper triangular form.  Now we work in the reverse direction to eliminate the nonzeros in the upper triangular part of the matrix.  We have \n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & -3 & 2 & 1 & 0 & 0 \\\\ \n",
    "0 & 1 & -1 & 2 & 1 & 0\\\\ \n",
    "0 & 0 &  1 & -8 & -2 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "R_1 \\leftarrow R_1 - 2R_3 \\\\\n",
    "R_2 \\leftarrow R_2 + R_3\\\\\n",
    "~\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & -3 & 0 & 17 & 4 & -2 \\\\ \n",
    "0 & 1 &  0 & -6 & -1 & 1\\\\ \n",
    "0 & 0 &  1 & -8 & -2 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "R_1 \\leftarrow R_1 +3R_2 \\\\\n",
    "~\\\\\n",
    "~\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & 0 & 0 & -1 & 1 & 1 \\\\ \n",
    "0 & 1 &  0 & -6 & -1 & 1\\\\ \n",
    "0 & 0 &  1 & -8 & -2 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "So $A^{-1} = \\mymat{-1 & 1 & 1 \\\\ -6 & -1 & 1 \\\\ -8 & -2 & 1}$.  We can check this by multiplying by $A$ and seeing if we get the identity matrix\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "AA^{-1} =\n",
    "\\mymat{\n",
    "1 & -3 & 2 \\\\ \n",
    "-2 & 7 & -5 \\\\ \n",
    "4 & -10 & 7\n",
    "}\n",
    "\\mymat{\n",
    "-1 & 1 & 1 \\\\ \n",
    "-6 & -1 & 1 \\\\ \n",
    "-8 & -2 & 1\n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "-1+18-16 & 1+3-4 & 1-3+2 \\\\\n",
    "2-42+40 & -2-7+10 & -2+7-5 \\\\\n",
    "-4+60-56 & 4+10-14 & 4-10+7\n",
    "} = \n",
    "\\mymat{\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "}\n",
    "\\quad \\checkmark\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "###Example 2\n",
    "\n",
    "Find $A^{-1}$ for $A = \\mymat{2 & 0 & 1 \\\\ 4 & 0 & 4 \\\\ -2 & 1 & -2}$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "2 & 0 & 1 & 1 & 0 & 0\\\\ \n",
    "4 & 0 & 4 & 0 & 1 & 0\\\\ \n",
    "-2 & 1 & -2 & 0 & 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "~\\\\\n",
    "R_2 \\leftarrow R_2 - 2R_1 \\\\\n",
    "R_3 \\leftarrow R_3 + R_1\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "2 & 0 & 1 & 1 & 0 & 0\\\\ \n",
    "0 & 0 & 2 & -2 & 1 & 0\\\\ \n",
    "0 & 1 & -1 & 1 & 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "~\\\\\n",
    "R_2 \\leftrightarrow R_3 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "2 & 0 & 1 & 1 & 0 & 0\\\\ \n",
    "0 & 1 & -1 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 2 & -2 & 1 & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "R_1 \\leftarrow R_1 -\\frac{1}{2}R_3\\\\\n",
    "R_2 \\leftarrow R_2 + \\frac{1}{2}R_3\\\\\n",
    "~\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "2 & 0 & 0 & 2 & -1/2 & 0\\\\ \n",
    "0 & 1 & 0 & 0 & 1/2 & 1 \\\\\n",
    "0 & 0 & 2 & -2 & 1 & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "R_1 \\leftarrow \\frac{1}{2}R_1\\\\\n",
    "~\\\\\n",
    "R_3 \\leftarrow \\frac{1}{2}R_3\n",
    "\\end{array}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{rrr|rrr}\n",
    "1 & 0 & 0 & 1 & -1/4 & 0\\\\ \n",
    "0 & 1 & 0 & 0 & 1/2 & 1 \\\\\n",
    "0 & 0 & 1 & -1 & 1/2 & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "So we have $A^{-1} = \\mymat{1 & -1/4 & 0 \\\\ 0 & 1/2 & 1 \\\\ -1 & 1/2 & 0}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that the Gauss-Jordan method explicitly constructs the inverse of $A$ on the *right-hand side* but constructing a matrix such that $AA^{-1} = I$.  But it implicitly constructs the inverse matrix on the left as well.  To see this remember that Gaussial Elimination can be viewed as multiplying on the left of $A$ by a sequence of elimination and permutation matrices.  We can view Gauss-Jordan elimination in the same way, but we have to include the row scaling as well.  In the end we have something like \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "(D \\cdots E \\cdots P \\cdots E)~A = I \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The collection of row-reduction matrices on the left of $A$ **is** $A^{-1}$.  \n",
    "\n",
    "<br>\n",
    "\n",
    "###Special Case: Inverses of Permutation Matrices\n",
    "\n",
    "Consider performing Gaussian-Elimination on a $3 \\times 3$ matrix $A$.  Row swaps in the elimination process can be viewed as multiplication by permutation matrices.  We've already see elementary row-swap matrices like \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P_{32} = \\mymat{1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Recall that multiplcation on the left by $P_{32}$ swaps the second and third row of the matrix or vector that it's applied to.  If we go behond elementary row swaps we can think about permutation matrices that scramble the order of the rows of a matrix into any order that we like.  If we stick with $3 \\times 3$ matrices then it's easy to see that there are **six** different possible orderings of the rows.  These six reorderings are characterized by the following six permutation matrices. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "I = \\mymat{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1}, \\quad \\quad \n",
    "P_{21} = \\mymat{0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1}, \\quad\\quad\n",
    "~~~~~~ \\mymat{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P_{31} = \\mymat{0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0}, \\quad \\quad \n",
    "P_{32} = \\mymat{1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0}, \\quad\\quad\n",
    "~~~ \\mymat{0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identity matrix is the trivial ordering.  Three of the matrices are the elementary row swap matrices that we've already seen.  And the last two are new, not corresponding to exactly one row exchange.  Let's look at the familiar cases first.  \n",
    "\n",
    "What is the inverse of the matrix $P_{21}$?  Remember that the inverse matrix should undo whatever the original matrix did.  Clearly, if $P_{21}$ swaps rows 1 and 2 of a matrix, then applying $P_{21}$ again would swap them back.  So $P_{21}$ is it's own inverse.  Clearly this works with the other elementary row swap matrices as well, so we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P_{21}^{-1} = P_{21} \\quad \\quad P_{31}^{-1} = P_{31} \\quad \\quad P_{32}^{-1} = P_{32} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "But what about those other two?  Let's look at the first one \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P = \\mymat{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "and check to see if it also is it's own inverse.  We have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "PP = \n",
    "\\mymat{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0}\n",
    "\\mymat{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0}\n",
    "= \n",
    "\\mymat{\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "}\n",
    "\\neq I \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "OK, clearly that didn't work.  It might help if we can figure out how to write the matrix $P$ as a product of the elementary row swap matrices.  Upon inspection, it's clear that we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P = P_{31}P_{32}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Since we know the inverse of each of the elementary permutation matrices, and we know the rule about the inverse of products, we can compute the inverse of $P$.  We have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P^{-1} = (P_{31}P_{32})^{-1} = P_{32}^{-1}P_{31}^{-1} = P_{32}P_{31} =\n",
    "\\mymat{\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "OK, so in general the inverse of a permutation matrix is not itself.  But if we look closely at the example we just did, we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mymat{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0}^{-1} = \n",
    "\\mymat{\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that the inverse of the permutation matrix is obtained by swapping the roles of the columns and rows of the matrix.  The operation of exchanging the rows and columns of a matrix is called the **matrix transpose** and is extremely useful in linear algebra.  \n",
    "\n",
    "Some things about the transpose operation\n",
    "\n",
    "* Swaps rows and columns of a matrix\n",
    "* The transpose of $A$ is denoted $A^T$\n",
    "* If $A$ is $m \\times n$ then $A^T$ is $n \\times n$\n",
    "* The entries of $A^T$ are given by $\\left[A^T\\right]_{ij} = A_{ji}$\n",
    "\n",
    "###Example 3 \n",
    "$$\n",
    "A = \\mymat{1 & 2 & 3 \\\\ 4 & 5 & 6} \\quad \\quad \\Rightarrow \\quad \\quad A^T = \\mymat{1 & 4 \\\\ 2 & 5 \\\\ 3 & 6}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that in general if $P$ is a permutation matrix then $P^T = P^{-1}$.  This does not conflict with our observation that the elementary row swap matrices are their own inverses.  Note, for example, for $P_{21}$ we have\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P_{21}^T = \n",
    "\\mymat{\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "}^T = \n",
    "\\mymat{\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "} = P_{21} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "When a matrix is equal to it's own transpose, i.e. $A = A^T$, we say the matrix $A$ is **symmetric**. \n",
    "\n",
    "**Arithmetic Rules for Matrix Transposes**\n",
    "\n",
    "1. $(A + B)^T = A^T + B^T$\n",
    "2. $(AB)^T = B^TA^T$\n",
    "3. $(A^{-1})^T = (A^T)^{-1}$\n",
    "\n",
    "**Proof of 2**\n",
    "\n",
    "Recall that $[AB]_{ij} = (row ~~ i ~~ of ~~ A) \\cdot (col ~~ j ~~ of ~~ B)$, then \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "[(AB)^T]_{ij} = (row ~~ j ~~ of ~~ A) \\cdot (col ~~ i ~~ of ~~ B)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "[B^TA^T]_{ij} = (row ~~ i ~~ of ~~ B^T) \\cdot (col ~~ j ~~ of ~~ A^T) = (col ~~ i ~~ of ~~ B) \\cdot (row ~~ j ~~ of ~~ A) = [(AB)^T]_{ij}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<br> \n",
    "\n",
    "**Proof of 3** \n",
    "\n",
    "We start with the obvious expression of the fact that the identity matrix $I$ is symmetric\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "I^T = I \\quad \\Rightarrow \\quad (AA^{-1})^T=I \\quad \\Rightarrow \\quad (A^{-1})^T A^T = I \\quad \\Rightarrow \\quad (A^T)^{-1} = (A^{-1})^T\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are some fun facts that are true based on the material in this lecture.  See if you can prove them on your own! \n",
    "\n",
    "<br>\n",
    "\n",
    "**Fun Fact #1**: For any $m \\times n$ matrix $A$, the products $A^TA$ and $AA^T$ are square and symmetric. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Fun Fact #2**: If $A$ is invertible and you permute the rows of $A$ to get a new matrix $B$, then $B$ is guaranteed to be invertible as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
