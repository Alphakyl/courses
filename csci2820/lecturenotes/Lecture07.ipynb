{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mymat}[1]{\n",
    "\\left[\n",
    "\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right]\n",
    "}\n",
    "\\newcommand{\\myaug}[1]{\n",
    "\\left[\n",
    "\\begin{array}{rrr|r}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right]\n",
    "}\n",
    "\\newcommand{\\myp}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\myb}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\myv}[1]{\\left< #1 \\right>}\n",
    "\\newcommand{\\mys}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\myab}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\bx}{{\\bf x}}\n",
    "\\newcommand{\\by}{{\\bf y}}\n",
    "\\newcommand{\\bu}{{\\bf u}}\n",
    "\\newcommand{\\bv}{{\\bf v}}\n",
    "\\newcommand{\\be}{{\\bf e}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 7\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "In this lecture we'll examine the computational costs associated with the elementry matrix and vector operations we've studied so far in the course.  Keep in mind though that raw computational cost is not the only thing that determines the performance of an algorithm.  In the next lecture we'll look at how memory access of an algorithm can play a large role in the overall performance of an implementation.  \n",
    "\n",
    "Recall that a real $m \\times n$ matrix $A$ has $m$ rows and $n$ columns, and we refer to the entry in the $i^{\\textrm{th}}$ row and $j^{\\textrm{th}}$ column of the matrix as $a_{ij}$. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\mymat{\n",
    "a_{11} & a_{12} & & \\cdots & & a_{1n} \\\\\n",
    "a_{21} & a_{22} & & \\cdots & & a_{2n} \\\\\n",
    "\\vdots & \\vdots & &        & & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & & \\cdots & & a_{mn} \\\\\n",
    "}\n",
    "\\quad\\quad\\quad\\quad\\quad\n",
    "\\bx = \n",
    "\\mymat{\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "}\n",
    "\\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "It's useful to think of matrix-vector products and matrix-matrix products as combinations of dot products, so we'll look at the dot product first. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Dot Product**: Let $x, y \\in \\mathbb{R}^n$, then \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx \\cdot \\by = \\bx^T\\by = \\sum_{k=1}^n x_ky_k = x_1y_1 + x_2y_2 + \\cdots + x_ny_n\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "If we do an operation count we see that the dot product for two n-length vectors requires $n$ **multiplies** and $(n-1)$ **additions**.  \n",
    "\n",
    "On typical modern architectures there is only a slight difference between the time it takes to multiply or add two floating-point numbers, so we'll lump these two operations together and refer to them as FLOPS (floating-point operations). \n",
    "\n",
    "So the dot product requires about $2n-1$ FLOPS, which for simplicitly we'll round to $2n$ FLOPS. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Matrix-Vector Product**: Consider a real $m \\times n$ matrix $A$ and an n-length vector $\\bx$. \n",
    "\n",
    "Recall that in the row-oriented view of matrix-vector multiplication we computed the $i^{\\textrm{th}}$ entry of the resulting m-length vector ${\\bf b} = A\\bx$ as the dot product the $i^{\\textrm{th}}$ row of $A$ with the vector $\\bx$. Then the total cost of the mat-vec is \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "m \\textrm{ dot products @ } 2n \\textrm{ FLOPS each } = 2mn \\textrm{ FLOPS}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that if $A$ is a square $n \\times n$ matrix the matrix-vector product is a $2n^2$ FLOP operation. \n",
    "\n",
    "Another useful way of computing the number of FLOPS required by an algorithm is by writing the algorithm out in psuedo-code.  Here is one way to compute a row-wise matrix-vector product: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = zeros(m)\n",
    "for ii=1:m\n",
    "    for jj=1:n\n",
    "        b[ii] = b[ii] + A[ii,jj]*x[jj]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the outer loop loops over the entries of the vector ${\\bf b}$ while the inner loop loops over the $i^{\\textrm{th}}$ row of $A$ and computes the dot product with $\\bx$. To use the pseudocode to compute the FLOP count we count the number of FLOPS done in the inner-most loop, and then turn the loops into sums.  The operation in the inner-most loop requires 2 FLOPS (1 multiply and 1 add).  So we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m \\sum_{j=1}^n 2 = \\sum_{i=1}^m 2n = 2mn \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Matrix-Matrix Product**: Consider a real $m \\times p$ matrix $A$ and a real $p \\times n$ matrix $B$.  The resulting product $AB$ has dimension $m \\times n$. \n",
    "\n",
    "Recall that the way we thought about matrix-matrix products was that the (i,j)-entry of the product was the dot product of row i of $A$ and col j of $B$.  Since there are $mn$ such entries in $AB$ the cost is \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "mn \\textrm{ dot products @ } 2p \\textrm{ FLOPS each } = 2mnp \\textrm{ FLOPS}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "And if $A$ and $B$ are both square $n \\times n$ matrices then the mat-mat product costs $2n^3$ FLOPS. \n",
    "\n",
    "If we go the pseudocode route we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = zeros(m,n)\n",
    "for ii=1:m\n",
    "    for jj=1:n\n",
    "        for kk=1:p\n",
    "            C[ii,jj] = C[ii,jj] + A[ii,kk]*B[kk,jj]\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation in the inner-most loop again requires 2 FLOPS.  Turning the loops into summations we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m\\sum_{j=1}^n\\sum_{k=1}^p 2 = 2mnp \\textrm{ FLOPS}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Forward Substitution**: Recall that Forward Substition is a method for solving linear systems of the form $L\\bx = {\\bf b}$ where the matrix $L$ is $n \\times n$ and lower triangular. Written out in system form, the system and the solution process look as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrrrrrr}\n",
    "\\ell_{11}x_1 && && && && = & b_1 \\\\\n",
    "\\ell_{21}x_1 &+& \\ell_{22}x_2 &&&&&& = & b_2 \\\\\n",
    "\\ell_{31}x_1 &+& \\ell_{32}x_2 &+& \\ell_{33}x_3 &&&& = & b_n \\\\\n",
    " & & & \\vdots & & & & & & \\\\\n",
    "\\ell_{n1}x_1 &+& \\ell_{n2}x_2 &+& \\ell_{n3}x_3 &+& \\cdots &+& \\ell_{nn}x_n = & b_n \n",
    "\\end{array}\n",
    "\\quad\\quad\\quad\n",
    "\\begin{array}{l}\n",
    "x_1 = b_1 / \\ell_{11} \\\\\n",
    "x_2 = \\myp{b_2 - \\ell_{21}x_1} / \\ell_{22} \\\\\n",
    "x_3 = \\myp{b_3 - \\ell_{31}x_1 - \\ell_{32}x_2} / \\ell_{33} \\\\\n",
    "~\\vdots \\\\\n",
    "x_n = \\myp{b_n - \\ell_{n1}x_1 - \\cdots - \\ell_{n,n-1}x_{n-1}} / \\ell_{nn}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Pseudocode for the Forward Substitution process looks as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii=1:n\n",
    "    x[ii] = b[ii]\n",
    "    for jj=1::ii-1\n",
    "        x[i] = x[i] - L[ii,jj]*x[jj]\n",
    "    end\n",
    "    x[ii] = x[ii]/L[ii,ii]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main brunt of the work occurs inside the two nested loops.  For now we'll ignore assignment and the divide that occur outside of the inner loop.  The operation inside the inner-most loop again requires 2 FLOPS.  Writing the loops as sums we then have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\sum_{j=1}^{i-1} 2 = 2\\sum_{i=1}^n (i-1) = 2 \\myp{ \\frac{n(n+1)}{2} - n} = n^2-n \\approx n^2 \\textrm{ FLOPS}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "OK, so the most expensive part of Forward Substitution costs $n^2$ FLOPS.  The assignment operation at the beginning requires $n$ array accesses and $n$ assignments.  The division at the end occurs $n$ times.  Despite the fact that division is between 2 and 8 times as expensive as multiplication and addition, the fact that these things occur only $n$ times is dwarfed by the $n^2$ operations in the inner-most loop.  As a result we say the cost of Forward Substitution is about $n^2$ FLOPS. \n",
    "\n",
    "It should be no surprise that Back Substitution also costs about $n^2$ FLOPS, since it requires the same operations as Forward Substition, just in the reverse order. \n",
    "\n",
    "<br> \n",
    "\n",
    "**LU Decomposition**: Again assume that $A$ is a real $n \\times n$ matrix given by \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\mymat{\n",
    "a_{11} & a_{12} & & \\cdots & & a_{1n} \\\\\n",
    "a_{21} & a_{22} & & \\cdots & & a_{2n} \\\\\n",
    "\\vdots & \\vdots & &        & & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & & \\cdots & & a_{mn} \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The elimination phase of the LU Decomposition proceeds column-by-column and uses the pivot on the diagonal to eliminate the entries in the column below it.  Let's find the cost of just the first phase of elimination where we use the pivot in the (1,1)-position to eliminate everything below it.  \n",
    "\n",
    "**to eliminate column 1**:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "           & \\textrm{mults} & \\textrm{adds} & \\textrm{total} \\\\\n",
    "a_{21}: R_2 \\leftarrow R_2 - \\ell_{21}R_1 & n-1 & n-1 & 2(n-1) \\\\\n",
    "a_{31}: R_3 \\leftarrow R_3 - \\ell_{31}R_1 & n-1 & n-1 & 2(n-1) \\\\\n",
    "\\quad\\vdots & \\quad\\vdots & \\quad\\vdots & \\quad\\vdots \\\\\n",
    "a_{n1}: R_n \\leftarrow R_n - \\ell_{n1}R_1 & n-1 & n-1 & 2(n-1) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We need to do this elimination for each of the $(n-1)$ entries below the pivot in the first column for a grand total of $2(n-1)^2$ FLOPS. \n",
    "\n",
    "Note that in counting the number of each operations I'm ignoring a few things.  First, I'm ignoring the computation of the multiplier $\\ell{i1}$ which is obtained by dividing the element to eliminate by the pivot. This is a total of $(n-1)$ divides, which is dwarfed by the approximately $n^2$ flops done for the row operations.  I'm also ignoring the the row operations on the actual entries in the first column.  There is no need, for example, to subtract a multiple of $a_{11}$ from $a_{21}$ to zero it out.  We picked the multplier exactly to accomplish this.  \n",
    "\n",
    "OK, so to eliminate the first column in the matrix requires about $2(n-1)^2$ FLOPS.  Now we need to use the pivot in the (2,2)-position to eliminate everything below it in the second column.  This looks as follows: \n",
    "\n",
    "**to eliminate column 1**:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "           & \\textrm{mults} & \\textrm{adds} & \\textrm{total} \\\\\n",
    "a_{32}: R_3 \\leftarrow R_3 - \\ell_{32}R_2 & n-2 & n-2 & 2(n-2) \\\\\n",
    "a_{42}: R_4 \\leftarrow R_4 - \\ell_{42}R_2 & n-2 & n-2 & 2(n-2) \\\\\n",
    "\\quad\\vdots & \\quad\\vdots & \\quad\\vdots & \\quad\\vdots \\\\\n",
    "a_{n2}: R_n \\leftarrow R_n - \\ell_{n2}R_2 & n-2 & n-2 & 2(n-2) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Since we have to do these row operations on $(n-2)$ entries in column 2 this gives a grand total of $2(n-2)^2$ FLOPs. \n",
    "\n",
    "OK, by now we can see the pattern.  Eliminating the third column is going to require $2(n-3)^2$ flops and so on.  Summing these up we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "2(n-1)^2 + 2(n-2)^2 + \\cdots 2(1)^2 = 2\\sum_{k=1}^n k^2 = 2\\myb{\\frac{(n-1)n(2n-1)}{6}} \\approx \\frac{2}{3}n^3 \\textrm{ FLOPs}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**The Integral Trick**: If you have complicated summation formulas that you need to evaluate, and you really only care about the leading order term, you can replace the summation by an integra.  We have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "2\\sum_{k=1}^{n-1}k^2 \\approx 2\\sum_{k=1}^n \\approx 2\\int_0^n k^2 dk = \\left. \\frac{2}{3}k^3 \\right|_0^n = \\frac{2}{3}n^3\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "So the cost of computing the LU Decomposition is approximately $\\frac{2}{3}n^3$ FLOPs.  Now we're ready to approximate the cost of solving the linear system $A\\bx = {\\bf b}$ using LU Decomposition and Forward/Back Substitition.  \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\textrm{LU} & \\approx & \\frac{2}{3}n^3 \\\\\n",
    "\\textrm{F Solve} & \\approx & n^2 \\\\\n",
    "\\textrm{B Solve} & \\approx & n^2 \\\\\n",
    "\\hline\n",
    "\\textrm{Total} & \\approx & \\frac{2}{3}n^3 + 2n^2 \\textrm{ FLOPs} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that this is the cost for solving $A\\bx = {\\bf b}$ for **one** rhs via LU Decomposition.  After computing the decomposition at a cost of roughtly $n^3$, each additional rhs vector that you want to solve with only costs an additional $2n^2$ FLOPs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Inverse**: Let's now compare this with finding the inverse matrix $A^{-1}$.  Recall that in class we said that the columns of the inverse matrix could be found by solving linear systems of the form \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bx_k = \\be_k \\textrm{   for   } k=1, \\ldots, n\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "An efficient strategy for doing this would be to compute the LU Decomposition of $A$ and then use it to solve for the $n$ rhs vectors using Forward and Back substitution.  If we do this the total cost is \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\frac{2}{3}n^3 + n\\myp{2n^2} = \\frac{8}{3}n^3 \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "So the total cost of finding the inverse is **FOUR TIMES** as expensive as compute the LU Decomposition. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
