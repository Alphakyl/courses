{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mymat}[1]{\n",
    "\\left[\n",
    "\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right]\n",
    "}\n",
    "\\newcommand{\\myaug}[1]{\n",
    "\\left[\n",
    "\\begin{array}{rrr|r}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right]\n",
    "}\n",
    "\\newcommand{\\mydet}[1]{\n",
    "\\left|\n",
    "\\begin{array}{rr|r}\n",
    "#1\n",
    "\\end{array}\n",
    "\\right|\n",
    "}\n",
    "\\newcommand{\\myp}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\myb}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\myv}[1]{\\left< #1 \\right>}\n",
    "\\newcommand{\\mys}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\myab}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\bx}{{\\bf x}}\n",
    "\\newcommand{\\by}{{\\bf y}}\n",
    "\\newcommand{\\bu}{{\\bf u}}\n",
    "\\newcommand{\\bv}{{\\bf v}}\n",
    "\\newcommand{\\bw}{{\\bf w}}\n",
    "\\newcommand{\\be}{{\\bf e}}\n",
    "\\newcommand{\\R}[1]{\\mathbb{R}^{ #1 }}\n",
    "\\newcommand{\\hs}{\\hspace{1mm}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 20\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Eigenvalues and eigenvectors of a square matrix $A$ play a huge role in many applications.  They are the basis for the famous Google PageRank algorithm, image compression, stochastic simulation, and many other applications in engineering and data science.  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Example 1**: Consider the matrix $A = \\mymat{3 & 1 \\\\ 1 & 3}$ and it's action on several vectors in $\\mathbb{R}^2$. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{1 \\\\ 2} = \\mymat{5 \\\\ 7}\n",
    "\\quad\\quad\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{0 \\\\ -2} = \\mymat{-2 \\\\ -6}\n",
    "\\quad\\quad\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{1 \\\\ 0} = \\mymat{3 \\\\ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{1 \\\\ 1} = \\mymat{4 \\\\ 4}\n",
    "\\quad\\quad\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{-1 \\\\ 1} = \\mymat{-2 \\\\ 2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that multiplication of the first three vectors in the example by $A$ change both the direction and the magnitude of the vector, while multiplication on the last two vectors by $A$ simply return scaled versions of the original vector. We have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{1 \\\\ 1} = \\mymat{4 \\\\ 4} = 4 \\mymat{1 \\\\ 1}\n",
    "\\quad\\quad\n",
    "\\mymat{3 & 1 \\\\ 1 & 3}\\mymat{-1 \\\\ 1} = \\mymat{-2 \\\\ 2} = 2\\mymat{-1 \\\\ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "When this happens we call the vector $\\bv$ an **eigenvector** or the matrix $A$, and the scalar $\\lambda$, an **eigenvalue** of the matrix $A$. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Definition**: Let $A$ be a *square* $n \\times n$ matrix.  A scalar $\\lambda$ is called an **eigenvalue** of $A$ if there is a **nonzero** vector $\\bv$, called an eigenvector, such that \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bv = \\lambda \\bv\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**: Can you guess an eigenvector of the matrix $B = \\mymat{1 & 2 \\\\ 1 & 2}$? \n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that the columns of $B$ are linearly dependent, and the nullspace of $B$ contains the vector $\\bv = \\mymat{2 \\\\ -1}$. \n",
    "\n",
    "<br>\n",
    "\n",
    "Multiplying $B$ by $\\bv$ gives \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mymat{1 & 2 \\\\ 1 & 2} \\mymat{2 \\\\ -1} = \\mymat{0 \\\\ 0} = 0\\mymat{2 \\\\ -1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Nullspace vectors of square matrices are eigenvectors with associated eigenvalues of $\\lambda = 0$. \n",
    "\n",
    "OK, so how do we find the other eigenvector and eigenvalue of $B$?  Typically we find all of the eigenvalues at the same time, and then use them to go looking for eigenvectors.  \n",
    "\n",
    "<br>\n",
    "\n",
    "Assume $\\myp{\\lambda, \\bv}$ are an eigenvalue and eigenvector of a matrix $A$ (together, we call them an **eigenpair** of $A$).  Then \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bv = \\lambda \\bv \\quad \\Rightarrow \\quad A\\bv - \\lambda \\bv = {\\bf 0} \\quad \\Rightarrow \\quad A\\bv -\\lambda I \\bv = {\\bf 0} \\quad \\Rightarrow \\quad \\myp{A-\\lambda I}\\bv = {\\bf 0}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Since if $\\bv$ is an eigenvector it is nonzero by definition, the above expression tells us that the matrix $A - \\lambda I$ is singular and $\\bv$ is in its nullspace. \n",
    "\n",
    "<br>\n",
    "\n",
    "Recall that a square matrix $A$ is singular if and only if its determinant is zero.  Thus we can find the eigenvalues of a matrix by forming $A - \\lambda I$ and setting its determinant equal to zero. Let's try this for the matrix $B$ in the previous example. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Example 3**: Find the eigenvalues of $B = \\mymat{1 & 2 \\\\ 1 & 2}$. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "B - \\lambda I = \\mymat{1 - \\lambda & 2 \\\\ 1 & 2 - \\lambda} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Taking the determinant and setting it equal to zero gives \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\det\\myp{B -\\lambda I} = \\det\\mymat{1 - \\lambda & 2 \\\\ 1 & 2-\\lambda} = 0 \n",
    "\\quad \\Rightarrow \\quad \n",
    "\\myp{1-\\lambda}\\myp{2-\\lambda} - 2 = 0\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\lambda^2 - 3\\lambda = 0 \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The polynomial $\\det\\myp{B - \\lambda I} = \\lambda^2 - 3\\lambda$ is called the **characteristic polynomial** of $B$ and its roots are the eignevalues of $B$.  We have\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\lambda^2 - 3\\lambda = 0 \\quad \\Rightarrow \\quad \\lambda\\myp{3-\\lambda} = 0\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Clearly the roots of the characteristic polynomial are $\\lambda_1 = 0$ and $\\lambda_2 = 3$.  These are the eigenvalues of $B$. \n",
    "\n",
    "<br>\n",
    "\n",
    "To find their associated eigenvectors we recall that if $\\lambda$ is an eigenvalue of $B$ then $\\myp{B-\\lambda I}\\bv = {\\bf 0}$ and $\\bv$ is a nullspace vector.  We find it in the usual way by row reduction.  We have\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\lambda_1 = 0$: $ ~~~ B - 0I = B = \\mymat{1 & 2 \\\\ 1 & 2} \\quad \\sim \\quad \\mymat{1 & 2 \\\\ 0 & 0} \\quad \\Rightarrow \\quad \\bv_1 = \\mymat{-2 \\\\ 1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\lambda_2 = 3$: $ ~~~ B - 3I = B = \\mymat{-2 & 2 \\\\ 1 & -1} \\quad \\sim \\quad \\mymat{-1 & 1 \\\\ 0 & 0} \\quad \\Rightarrow \\quad \\bv_2 = \\mymat{1 \\\\ 1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Sure enough we can check that if we multiply $B$ by $\\bv_2 = \\mymat{1 \\\\ 1}$ we get $\\bv_2$ back but scaled by $\\lambda_2 = 3$. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "B\\bv_2 = \\mymat{1 & 2 \\\\ 1 & 2}\\mymat{1 \\\\ 1} = \\mymat{3 \\\\ 3} = 3 \\mymat{1 \\\\ 1} \\quad \\checkmark\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "But wait a second.  When we guessed the nullspace vector of $B$ before, we said that $\\mymat{2 \\\\ -1}$ should be the eigenvector associated with the zero eigenvalue. \n",
    "\n",
    "<br>\n",
    "\n",
    "It turns out that eigenvectors are equivalent up to nonzero scalar multiplications.  We can see this since if $\\bv$ is an eigenvector with associated eigenvalue $\\lambda$ then $A\\bv = \\lambda \\bv$.  But, so is $\\alpha \\bv$ where $\\alpha$ is a nonzer scalar: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\myp{\\alpha \\bv} = \\alpha A\\bv = \\alpha \\lambda \\bv = \\lambda \\myp{\\alpha \\bv} \\quad \\checkmark\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "It turns out that you saw an example of an eigenvector and eigenvalue way back in Homework 1 when you were practicing matrix vector multiplication.  Consider the following application. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Application**: Consider a town with two stores, thoughtfully named Store 1 and Store 2.  Suppose that shoppers in the town follow the following rules.  If a person visits Store 1 in one week then there is a 60% chance that they'll visit Store 1 again the following week and a 40% chance that they'll switch to Store 2.  Similarly, if a person visits Store 2 in one week then there is an 80% chance that they'll stick with Store 2 in the following week and a 20% chance they'll switch to Store 1.  \n",
    "\n",
    "Suppose that you're looking to invest in one of the two grocery stores, and you want to know which store will be more popular in the long run.  We can encode the week-to-week transition rules into a matrix and then run a simple simulation to see what shopping habbits will be like in the distanct future. \n",
    "\n",
    "Let $\\bx_k = \\mymat{x_1 \\\\ x_2}$ be a state vector that represents the fraction of shoppers that will shop at the two stores in week $k$.\n",
    "\n",
    "Then according the transition rules, the number of people that will shop at the store in the next week can be computed as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "x_1 \\mymat{0.6 \\\\ 0.4} + x_2 \\mymat{0.2 \\\\ 0.8} = x_1 \\mymat{3/5 \\\\ 2/5} + x_2 \\mymat{1/5 \\\\ 4/5} = \\mymat{3/5 & 1/5 \\\\ 2/5 & 4/5}\\mymat{x_1 \\\\ x_2} = M\\bx \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The matrix $M$ is called a *Markov Matrix*, a *Transition Matrix*, or sometimes a *Stochastic Matrix*. It's defining quality is that it has all nonnegative entries and it's columns sum to 1 (because the columns represent probability distributions). Then, starting from some initial state at week 0, $\\bx_0$, we can predict the state of the system at week 1 by multiplying $\\bx_0$ by $M$: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_1 = M\\bx_0 \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Similarly, if we want to predict the percentage of shoppers at each store we can multiply $\\bx_1$ by $M$.  Notice though that this is equivalent to multiplying $\\bx_0$ by $M^2$. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_2 = M\\bx_1 = M^2\\bx_0 \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "And if we want to know the fraction of shoppers at each store in week 100 we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_{100} = M\\bx_{99} = M^{100}\\bx_0\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's run the simulation for 50 weeks starting with the initial state vector $\\bx_0 = \\mymat{1.0 \\\\ 0}$ indicating that all of the shopper go to Store 1 in the opening week. We'll also print the results of the simulation after every 5 weeks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34016,0.6598400000000001]\n",
      "[0.3334032384000001,0.6665967616000001]\n",
      "[0.3333340491612161,0.6666659508387843]\n",
      "[0.333333340663411,0.6666666593365896]\n",
      "[0.33333333340839355,0.6666666665916072]\n",
      "[0.33333333333410226,0.6666666666658987]\n",
      "[0.3333333333333415,0.6666666666666594]\n",
      "[0.33333333333333376,0.6666666666666673]\n",
      "[0.3333333333333337,0.6666666666666675]\n",
      "[0.3333333333333337,0.6666666666666675]\n"
     ]
    }
   ],
   "source": [
    "M = [0.6 0.2; 0.4 0.8]\n",
    "x = [1.0,0.0]\n",
    "for ii=1:50\n",
    "    x = M*x \n",
    "    if ii%5 == 0 \n",
    "        println(x)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be pretty clear that the vector $\\bx_k$ is approaching $\\bv = \\mymat{1/3 \\\\ 2/3}$.\n",
    "\n",
    "This means that in the long-term behavior of the system, one third of shoppers will shop at Store 1 and two thirds of shoppers will shop at Store 2.  This vector $\\bv$ is sometimes called the *stationary distribution* of the system. \n",
    "\n",
    "From a linear algebra perspectice this also means that multiplying the vector $\\bv = \\mymat{1/3 \\\\ 2/3}$ by $M$ leaves the vector unchanged.  \n",
    "\n",
    "This means that $\\bv$ is an eigenvector of $M$ with associated eigenvalue $\\lambda = 1$. \n",
    "\n",
    "But what about the other eigenvector and eigenvalue of $M$? Let's find them all algebraically. \n",
    "\n",
    "We want to find the eigenvalues first by finding the roots of the characteristic equation of $M$.  We have\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "0 = \\det\\myp{M - \\lambda I} &=& \\mydet{3/5 - \\lambda & 1/5 \\\\ 2/5 & 4/5 - \\lambda} \\\\\n",
    "&=& \\myp{3/5 - \\lambda}\\myp{4/5-\\lambda} - 2/25 \\\\\n",
    "&=& \\myp{3-5\\lambda}\\myp{4-5\\lambda} - 2 \\\\\n",
    "&=& 25\\lambda^2 - 35\\lambda + 10  \\\\\n",
    "&=& 25\\lambda^2 - 25\\lambda - 10\\lambda + 10  \\\\\n",
    "&=& 25\\lambda\\myp{\\lambda - 1} - 10\\myp{\\lambda - 1} \\\\\n",
    "&=& \\myp{25\\lambda - 10}\\myp{\\lambda - 1}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The eigenvalues of $M$ and roots of the characteristic polynomial are $\\lambda_1 = 1$ and $\\lambda_2 = 2/5$. \n",
    "\n",
    "To find the associated eigenvectors we find the nullspace components of $M - \\lambda I$.  We have\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\lambda_1 = 1$: $~~~ M-I = \\mymat{-2/5 & 1/5 \\\\ 2/5 & -1/5} \\quad \\sim \\quad \\mymat{-2/5 & 1/5 \\\\ 0 & 0} \\quad \\Rightarrow \\quad \\hat{\\bv}_1 = \\mymat{1 \\\\ 2} \\quad \\Rightarrow \\quad \\bv_1 = \\mymat{1/3 \\\\ 2/3}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that here we chose to scale the eigenvector we found so that it's entries sum to 1 and it looks like a probability distribution.  Notice also that after scaling it's exactly the vector for the stationary distribution we found through simulation. For the second eigenvalue we have\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\lambda_2 = 2/5$: $~~~ M-2/5I = \\mymat{1/5 & 1/5 \\\\ 2/5 & 2/5} \\quad \\sim \\quad \\mymat{2/5 & 2/5 \\\\ 0 & 0} \\quad \\Rightarrow \\quad \\hat{\\bv}_1 = \\mymat{1 \\\\ -1} \\quad \\Rightarrow \\quad \\bv_2 = \\mymat{1/2 \\\\ -1/2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that the eigenvector associated with $\\lambda_1 = 1$ has positive entries (so it looks like a probability distribution) and the second eigenvector really doesn't.  This is typically the case for Markov matrices, where the first eigenvector is the only one with a statistical interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that the two eigenvectors are linearly independent and so form a basis for $\\mathbb{R}^2$.  This means that we can take the starting state of the system $\\bx_0$ and write it as a linear combination of the eigenvectors: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_0 = \\mymat{1 \\\\ 0} = 1\\mymat{1/3 \\\\ 2/3} + 4/3\\mymat{1/2 \\\\ -1/2} = 1\\bv_1 + 4/3\\bv_2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, remember that multiplication of an eigenvector by $M$ returns the original vector scaled by it's eigenvalue, we can iteration through weeks of the simulation very easily.  We have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_1 = M\\bx_0 = \\mymat{3/5 & 1/5 \\\\ 2/5 & 4/5} \\mymat{1 \\\\ 0} = \\mymat{3/5 & 1/5 \\\\ 2/5 & 4/5}\\myp{ \\mymat{1/3 \\\\ 2/3} + 4/3\\mymat{1/2 \\\\ -1/2}} = (1)\\mymat{1/3 \\\\ 2/3} + 4/3(2/5)\\mymat{1/2 \\\\ -1/2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Similarly, for the second week, we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_2 = M\\bx_1 = \\mymat{3/5 & 1/5 \\\\ 2/5 & 4/5}\\myp{ (1)\\mymat{1/3 \\\\ 2/3} + 4/3(2/5)\\mymat{1/2 \\\\ -1/2}} = (1)^2\\mymat{1/3 \\\\ 2/3} + 4/3(2/5)^2\\mymat{1/2 \\\\ -1/2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "And for Week 100 we have\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\bx_{100} = M\\bx_{99} = (1)^{100}\\mymat{1/3 \\\\ 2/3} + 4/3(2/5)^{100}\\mymat{1/2 \\\\ -1/2}= \\mymat{1/3 \\\\ 2/3} + 4/3(2/5)^{100}\\mymat{1/2 \\\\ -1/2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "It should now be clear why the eigenvector $\\bv_1$ associated with eigenvalue $\\lambda_1 =1$ emerges as the stationary trend of the system.  The second eigenvector has an eigenvalue $\\lambda_2 = 2/5$ which is less than 1.  The action of multiplying the current state vector by the Markov matrix is to raise the eigenvalues to a higher powers, and $\\lambda_2^k$ is an increasingly smaller and smaller number. In other words, the *dominant* eigenvector $\\bv_1$ hangs around, and the second eigenvector $\\bv_2$ gets dampened out.  For this reason, the eigenvector $\\bv_2$ is often called a *transient* mode of the simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding eigenvalues and eigenvectors of a $3 \\times 3$ matrix is exactly the same process as for $2 \\times 2$ matrices.  It's slighly more involved though because now we're looking for three eigenvalues and three eigenvectors. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Example 4**: Find the eigenvalues and eigenvectors of $A = \\mymat{1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Finding the roots of the characteristic equation, we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "0 = \\det \\mymat{1-\\lambda & -1 & 0 \\\\ -1 & 2-\\lambda & -1 \\\\ 0 & -1 & 1-\\lambda}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We'll perform co-factor expansion along the first row of the matrix, in order to exploit the zero in the (1,3) position. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "0 = \\det \\mymat{1-\\lambda & -1 & 0 \\\\ -1 & 2-\\lambda & -1 \\\\ 0 & -1 & 1-\\lambda} = \\myp{1-\\lambda}\\mydet{2-\\lambda & -1 \\\\ -1 & 1-\\lambda} - (-1)\\mydet{-1 & -1 \\\\ 0 & 1-\\lambda}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "0 = \\myp{1-\\lambda} \\myb{\\myp{2-\\lambda}\\myp{1-\\lambda} - 1} - \\myp{1-\\lambda} = \\myp{1-\\lambda}\\myb{\\lambda^2 - 3\\lambda} = \\lambda\\myp{1-\\lambda}\\myp{\\lambda-3}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The the three eigenvalues of $A$ are $\\lambda_1 = 3$, $\\lambda_2 = 1$ and $\\lambda_3 = 0$.\n",
    "\n",
    "Finding the eigenvectors, we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\lambda_1 = 3: ~~~ A-3I = \n",
    "\\mymat{\n",
    "-2 & -1 & 0 \\\\\n",
    "-1 & -1 & -1 \\\\\n",
    "0 & -1 & -2}\n",
    "\\quad \\sim \\quad\n",
    "\\mymat{\n",
    "1 & 0 & -1 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "0 & 0 & 0}\n",
    "\\quad \\Rightarrow \\quad \\bv_1 = \\mymat{1 \\\\ -2 \\\\ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\lambda_2 = 1: ~~~ A-I = \n",
    "\\mymat{\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & -1 & -1 \\\\\n",
    "0 & -1 & 0}\n",
    "\\quad \\sim \\quad\n",
    "\\mymat{\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0}\n",
    "\\quad \\Rightarrow \\quad \\bv_2 = \\mymat{-1 \\\\ 0 \\\\ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\lambda_3 = 0: ~~~ A -0I = \n",
    "\\mymat{\n",
    "1 & -1 & 0 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "0 & -1 & 1}\n",
    "\\quad \\sim \\quad\n",
    "\\mymat{\n",
    "1 & -1 & 0 \\\\\n",
    "0 & 1 & -1 \\\\\n",
    "0 & 0 & 0}\n",
    "\\quad \\Rightarrow \\quad \\bv_3 = \\mymat{1 \\\\ 1 \\\\ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, checking these, we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bv_1 = \n",
    "\\mymat{\n",
    " 1 &-1 & 0 \\\\ \n",
    "-1 & 2 &-1 \\\\ \n",
    " 0 &-1 & 1}\n",
    "\\mymat{\n",
    " 1 \\\\\n",
    "-2 \\\\\n",
    " 1\n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "3 \\\\\n",
    "-6 \\\\\n",
    "3 \n",
    "} = 3 \\mymat{1 \\\\ -2 \\\\ 1} \\quad \\checkmark\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bv_2 = \n",
    "\\mymat{\n",
    " 1 &-1 & 0 \\\\ \n",
    "-1 & 2 &-1 \\\\ \n",
    " 0 &-1 & 1}\n",
    "\\mymat{\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "1 \n",
    "} = 1 \\mymat{-1 \\\\ 0 \\\\ 1} \\quad \\checkmark\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "A\\bv_3 = \n",
    "\\mymat{\n",
    " 1 &-1 & 0 \\\\ \n",
    "-1 & 2 &-1 \\\\ \n",
    " 0 &-1 & 1}\n",
    "\\mymat{\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "}\n",
    "= \n",
    "\\mymat{\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "} = 0 \\mymat{1 \\\\ 1 \\\\ 1} \\quad \\checkmark\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
